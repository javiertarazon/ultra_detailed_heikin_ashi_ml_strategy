#!/usr/bin/env python3
"""
Pipeline de Optimizaci√≥n Completo para UltraDetailedHeikinAshi

Este script implementa el pipeline completo de optimizaci√≥n:
1. Entrena modelos ML con datos hist√≥ricos
2. Optimiza par√°metros de la estrategia con Optuna
3. Ejecuta un backtest final con los mejores par√°metros

Pasos:
- Entrenamiento ML: 2024 H1 (train), 2024 H2 (validation)
- Optimizaci√≥n: 2024 completo (datos espec√≠ficos de optimizaci√≥n)
- Backtest final: Mejor configuraci√≥n en todo 2024
"""

import sys, os
# A√±adir el directorio padre (descarga_datos) al path para importar m√≥dulos
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, parent_dir)

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import json
import time
from typing import Dict, List, Tuple
import dataclasses

from config.config_loader import load_config_from_yaml
# Importaciones lazy para evitar KeyboardInterrupt en Python 3.13
# from ml_trainer import MLTrainer  # Importado solo cuando se necesita
# from strategy_optimizer import StrategyOptimizer  # Importado solo cuando se necesita
from strategies.ultra_detailed_heikin_ashi_ml_strategy import UltraDetailedHeikinAshiMLStrategy
from utils.logger import setup_logger

logger = setup_logger(__name__)


class OptimizationPipeline:
    def __init__(self,
                 symbols=None,
                 timeframe="4h",
                 train_start="2022-01-01",
                 train_end="2023-06-30",
                 val_start="2022-07-01",
                 val_end="2023-12-31",
                 opt_start="2022-01-01",
                 opt_end="2023-12-31",
                 n_trials=300):
        """
        Inicializa el pipeline de optimizaci√≥n completo.

        Args:
            symbols: Lista de s√≠mbolos a procesar
            timeframe: Timeframe para los datos
            train_start/end: Per√≠odo de entrenamiento ML
            val_start/end: Per√≠odo de validaci√≥n ML
            opt_start/end: Per√≠odo para optimizaci√≥n
            n_trials: N√∫mero de pruebas para Optuna
        """
        self.symbols = symbols if symbols else ["BTC/USDT"]
        self.timeframe = timeframe
        self.train_start = train_start
        self.train_end = train_end
        self.val_start = val_start
        self.val_end = val_end
        self.opt_start = opt_start
        self.opt_end = opt_end
        self.n_trials = n_trials

        # Cargar configuraci√≥n
        self.config = load_config_from_yaml()
        logger.info(f"Pipeline inicializado para s√≠mbolos: {self.symbols}")
        logger.info(f"Timeframe: {timeframe}, Trials: {n_trials}")

    async def run_complete_pipeline(self):
        """
        Ejecuta el pipeline completo de optimizaci√≥n para todos los s√≠mbolos.
        """
        start_time = time.time()
        pipeline_results = {}

        for symbol in self.symbols:
            try:
                symbol_start = time.time()
                logger.info(f"=== INICIANDO PIPELINE PARA {symbol} ===")

                # Paso 1: Entrenamiento ML (opcional en modo seguro)
                await self._train_ml_models(symbol)

                # Paso 2: Optimizaci√≥n de par√°metros
                opt_results = self._optimize_strategy_parameters(symbol)

                # Paso 3: Backtest final con mejores par√°metros
                backtest_results = self._run_final_backtest(symbol, opt_results)

                # Guardar resultados de este s√≠mbolo
                pipeline_results[symbol] = {
                    "optimization_results": opt_results,
                    "backtest_results": backtest_results,
                    "execution_time": time.time() - symbol_start
                }

                logger.info(f"Pipeline para {symbol} completado en {(time.time() - symbol_start)/60:.2f} minutos")

            except Exception as e:
                logger.error(f"Error en el pipeline para {symbol}: {e}")
                import traceback
                traceback.print_exc()

        # Guardar resultados completos
        self._save_pipeline_results(pipeline_results)

        total_time = time.time() - start_time
        logger.info(f"Pipeline completo finalizado en {total_time/60:.2f} minutos")

        return pipeline_results

    async def _train_ml_models(self, symbol):
        """
        Entrena los modelos ML para el s√≠mbolo dado.

        Args:
            symbol (str): S√≠mbolo a entrenar
        """
        logger.info(f"Iniciando entrenamiento ML para {symbol}")

        # Verificar si est√° en modo seguro (Python 3.13 compatibility)
        safe_mode = False
        if hasattr(self.config, 'ml_training'):
            ml_training = self.config.ml_training
            if hasattr(ml_training, 'safe_mode'):
                safe_mode = ml_training.safe_mode
        
        if safe_mode:
            logger.info("üõ°Ô∏è MODO SEGURO ACTIVADO - Saltando entrenamiento ML")
            logger.info("Los modelos ML existentes ser√°n usados si est√°n disponibles")
            return {
                'ml_training_completed': False,
                'safe_mode': True,
                'message': 'Modo seguro activado - usando modelos existentes o sin ML'
            }

        # Importaci√≥n lazy de MLTrainer para evitar KeyboardInterrupt en Python 3.13
        try:
            from .ml_trainer import MLTrainer
            logger.info("MLTrainer importado correctamente")
        except KeyboardInterrupt:
            logger.error("KeyboardInterrupt durante importaci√≥n de MLTrainer")
            logger.error("Esto puede deberse a problemas de compatibilidad con Python 3.13 y sklearn")
            raise RuntimeError("No se puede importar MLTrainer. Considere usar Python 3.11 para ML")

        # Crear instancia del entrenador ML
        trainer = MLTrainer(symbol, self.timeframe)
        trainer.train_start = self.train_start
        trainer.train_end = self.train_end
        trainer.val_start = self.val_start
        trainer.val_end = self.val_end

        # Ejecutar entrenamiento
        try:
            training_results = await trainer.run()
            logger.info(f"Entrenamiento ML completado para {symbol}")
            return training_results
        except Exception as e:
            logger.error(f"Error durante entrenamiento ML: {e}")
            raise

    def _optimize_strategy_parameters(self, symbol, n_trials=None):
        """
        Optimiza los par√°metros de la estrategia usando Optuna.

        Args:
            symbol (str): S√≠mbolo a optimizar
            n_trials (int): N√∫mero de trials (opcional)

        Returns:
            dict: Resultados de la optimizaci√≥n
        """
        if n_trials is None:
            n_trials = self.n_trials

        logger.info(f"Iniciando optimizaci√≥n de par√°metros para {symbol} con {n_trials} trials")

        # Importaci√≥n lazy de StrategyOptimizer
        try:
            from .strategy_optimizer import StrategyOptimizer
            logger.info("StrategyOptimizer importado correctamente")
        except ImportError as e:
            logger.error(f"No se puede importar StrategyOptimizer: {e}")
            # Fallback: devolver par√°metros por defecto en formato correcto (None, [])
            return None, []

        # Crear optimizador
        optimizer = StrategyOptimizer(
            symbol=symbol,
            timeframe=self.timeframe,
            start_date=self.opt_start,
            end_date=self.opt_end,
            n_trials=n_trials
        )

        # Ejecutar optimizaci√≥n
        try:
            opt_results = optimizer.run_optimization()
            logger.info(f"Optimizaci√≥n completada para {symbol}")
            return opt_results
        except Exception as e:
            logger.error(f"Error durante optimizaci√≥n: {e}")
            # Fallback: devolver par√°metros por defecto en formato correcto
            return None, []

    def _run_final_backtest(self, symbol, opt_results):
        """
        Ejecuta el backtest final con los mejores par√°metros.

        Args:
            symbol (str): S√≠mbolo a testear
            opt_results (tuple): Tupla (study, pareto_trials) de optimizaci√≥n

        Returns:
            dict: Resultados del backtest
        """
        logger.info(f"Ejecutando backtest final para {symbol}")

        # Verificar que opt_results sea v√°lido
        if opt_results is None or not isinstance(opt_results, tuple):
            logger.error(f"opt_results inv√°lido: {opt_results}")
            return {
                "error": "No se pudo realizar optimizaci√≥n - par√°metros inv√°lidos",
                "symbol": symbol
            }

        # Extraer mejores par√°metros del frente de Pareto - SELECCIONAR POR M√ÅXIMO P&L
        study, pareto_trials = opt_results
        if pareto_trials and len(pareto_trials) > 0:
            # Seleccionar el trial con el m√°ximo P&L del frente de Pareto
            best_trial = max(pareto_trials, key=lambda t: t.values[3])  # values[3] es total_pnl
            best_params = best_trial.params
            best_pnl = best_trial.values[3]
            logger.info(f"Mejor trial seleccionado por P&L m√°ximo: ${best_pnl:.2f}")
            logger.info(f"Mejores par√°metros encontrados: {best_params}")
        else:
            logger.warning("No se encontraron trials en el frente de Pareto, usando par√°metros por defecto")
            best_params = {}

        # COMPARAR CON PAR√ÅMETROS ORIGINALES ANTES DE APLICAR
        logger.info("üîç Comparando par√°metros originales vs optimizados...")
        original_results = self._run_backtest_with_params(symbol, {}, "originales")
        optimized_results = self._run_backtest_with_params(symbol, best_params, "optimizados")

        # Comparar resultados
        original_pnl = original_results.get('total_pnl', 0)
        optimized_pnl = optimized_results.get('total_pnl', 0)

        logger.info(f"üîç COMPARACI√ìN DE PAR√ÅMETROS:")
        logger.info(f"   Par√°metros originales: P&L = ${original_pnl:.2f}")
        logger.info(f"   Par√°metros optimizados: P&L = ${optimized_pnl:.2f}")
        logger.info(f"   Mejora: ${optimized_pnl - original_pnl:.2f} ({((optimized_pnl/original_pnl-1)*100) if original_pnl != 0 else 0:.1f}%)")

        # DECISI√ìN: Usar optimizados solo si mejoran significativamente los originales
        improvement_threshold = 0.05  # 5% de mejora m√≠nima
        if optimized_pnl > original_pnl * (1 + improvement_threshold):
            logger.info(f"‚úÖ Usando par√°metros OPTIMIZADOS (mejora > {improvement_threshold*100:.0f}%)")
            final_params = best_params
            final_results = optimized_results
            final_results['params_source'] = 'optimized'
        else:
            logger.warning(f"‚ö†Ô∏è Par√°metros optimizados NO mejoran significativamente los originales. Usando ORIGINALES.")
            logger.warning(f"   Recomendaci√≥n: Revisar configuraci√≥n de optimizaci√≥n o aumentar n√∫mero de trials.")
            final_params = {}
            final_results = original_results
            final_results['params_source'] = 'original'

        # Guardar resultados de comparaci√≥n
        comparison_results = {
            'original_results': original_results,
            'optimized_results': optimized_results,
            'comparison': {
                'original_pnl': original_pnl,
                'optimized_pnl': optimized_pnl,
                'improvement': optimized_pnl - original_pnl,
                'improvement_pct': ((optimized_pnl/original_pnl-1)*100) if original_pnl != 0 else 0,
                'params_used': final_results['params_source']
            }
        }

        # Actualizar config solo si usamos par√°metros optimizados
        if final_results['params_source'] == 'optimized':
            self._update_config_with_optimized_params(symbol, final_params)

        # Agregar comparaci√≥n a resultados finales
        final_results['comparison'] = comparison_results['comparison']

        return final_results

    def _run_backtest_with_params(self, symbol, params, params_type="desconocidos"):
        """
        Ejecuta un backtest con par√°metros espec√≠ficos.

        Args:
            symbol (str): S√≠mbolo a testear
            params (dict): Par√°metros a usar
            params_type (str): Tipo de par√°metros para logging

        Returns:
            dict: Resultados del backtest
        """
        logger.info(f"Ejecutando backtest con par√°metros {params_type} para {symbol}")

        # Crear estrategia con par√°metros espec√≠ficos
        if isinstance(self.config, dict):
            strategy_config = self.config.copy()
            strategy_config.update(params)
        else:
            # Es un objeto Config - convertir a dict y actualizar
            strategy_config = dataclasses.asdict(self.config)
            strategy_config.update(params)

        strategy = UltraDetailedHeikinAshiMLStrategy(config=strategy_config)
        strategy._optimization_mode = True

        # Cargar datos para backtest desde SQLite
        from utils.storage import DataStorage
        storage = DataStorage()

        start_ts = int(pd.Timestamp(self.opt_start).timestamp()) if self.opt_start else None
        end_ts = int(pd.Timestamp(self.opt_end).timestamp()) if self.opt_end else None
        table_name = f"{symbol.replace('/', '_')}_{self.timeframe}"

        data = storage.query_data(table_name, start_ts=start_ts, end_ts=end_ts)

        if data is None or data.empty:
            logger.error(f"No hay datos para backtest con par√°metros {params_type}")
            return {"total_pnl": 0, "error": "No data available"}

        # Asegurar integridad de datos
        data['high'] = np.maximum(data[['open', 'close']].max(axis=1), data['high'])
        data['low'] = np.minimum(data[['open', 'close']].min(axis=1), data['low'])

        # Ejecutar backtest
        try:
            results = strategy.run(data, symbol, self.timeframe)
            logger.info(f"Backtest {params_type} completado: P&L = ${results.get('total_pnl', 0):.2f}")
            return results
        except Exception as e:
            logger.error(f"Error en backtest {params_type}: {e}")
            return {"total_pnl": 0, "error": str(e)}

    def _update_config_with_optimized_params(self, symbol, params):
        """
        Actualiza la configuraci√≥n con par√°metros optimizados.

        Args:
            symbol (str): S√≠mbolo
            params (dict): Par√°metros optimizados
        """
        try:
            # Cargar configuraci√≥n actual
            config = load_config_from_yaml()

            # Preparar par√°metros optimizados para el s√≠mbolo
            symbol_key = symbol.replace('/', '_').replace('.', '_')
            optimized_params = getattr(config.backtesting, 'optimized_parameters', {}) or {}

            # Actualizar par√°metros para este s√≠mbolo
            optimized_params[symbol_key] = params
            config.backtesting.optimized_parameters = optimized_params

            # Guardar configuraci√≥n actualizada
            import yaml
            config_path = Path("descarga_datos/config/config.yaml")

            # Convertir config a dict para guardar
            config_dict = dataclasses.asdict(config)

            with open(config_path, 'w') as f:
                yaml.dump(config_dict, f, default_flow_style=False, indent=2)

            logger.info(f"Configuraci√≥n actualizada con par√°metros optimizados para {symbol}")

        except Exception as e:
            logger.error(f"Error actualizando configuraci√≥n: {e}")

    def _get_default_parameters(self, symbol):
        """
        Devuelve par√°metros por defecto cuando la optimizaci√≥n falla.

        Args:
            symbol (str): S√≠mbolo

        Returns:
            dict: Par√°metros por defecto
        """
        return {
            'best_params': {
                'ml_threshold': 0.7,
                'stoch_overbought': 85,
                'stoch_oversold': 15
            },
            'best_value': 0.0,
            'optimization_completed': False,
            'fallback': True
        }

    def _save_pipeline_results(self, results):
        """
        Guarda los resultados del pipeline completo.

        Args:
            results (dict): Resultados a guardar
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = Path(__file__).parent.parent / "data" / "optimization_pipeline"
        results_dir.mkdir(parents=True, exist_ok=True)

        results_file = results_dir / f"pipeline_complete_{timestamp}.json"

        try:
            with open(results_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            logger.info(f"Resultados del pipeline guardados en: {results_file}")
        except Exception as e:
            logger.error(f"Error guardando resultados: {e}")

    def run_quick_test(self):
        """
        Ejecuta un test r√°pido del pipeline con 5 trials por s√≠mbolo
        """
        logger.info("üöÄ Iniciando test r√°pido del pipeline de optimizaci√≥n")

        start_time = time.time()
        test_results = {}

        for symbol in self.symbols:
            logger.info(f"Test r√°pido para {symbol}")
            symbol_start = time.time()

            try:
                # Paso 1: Entrenamiento ML r√°pido
                self._train_ml_models(symbol)

                # Paso 2: Optimizaci√≥n con solo 5 trials
                opt_results = self._optimize_strategy_parameters(symbol, n_trials=5)

                # Paso 3: Backtest final con mejores par√°metros
                backtest_results = self._run_final_backtest(symbol, opt_results)

                test_results[symbol] = {
                    "optimization_results": opt_results,
                    "backtest_results": backtest_results,
                    "execution_time": time.time() - symbol_start
                }

                logger.info(f"Test r√°pido para {symbol} completado en {(time.time() - symbol_start)/60:.2f} minutos")

            except Exception as e:
                logger.error(f"Error en test r√°pido para {symbol}: {e}")
                import traceback
                traceback.print_exc()

        # Guardar resultados del test
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = Path(__file__).parent.parent / "data" / "optimization_pipeline"
        results_dir.mkdir(parents=True, exist_ok=True)

        test_file = results_dir / f"quick_test_{timestamp}.json"

        try:
            with open(test_file, 'w') as f:
                json.dump(test_results, f, indent=2, default=str)
            logger.info(f"Resultados del test r√°pido guardados en: {test_file}")
        except Exception as e:
            logger.error(f"Error guardando resultados del test: {e}")

        total_time = time.time() - start_time
        logger.info(f"Test r√°pido completado en {total_time/60:.2f} minutos")

        return test_results


async def main():
    """
    Funci√≥n principal para ejecutar el pipeline desde l√≠nea de comandos.
    """
    import argparse

    parser = argparse.ArgumentParser(description='Pipeline de Optimizaci√≥n Completo')
    parser.add_argument('--symbols', nargs='+', default=['SOL/USDT'],
                        help='S√≠mbolos a optimizar')
    parser.add_argument('--timeframe', default='4h',
                        help='Timeframe para los datos')
    parser.add_argument('--trials', type=int, default=50,
                        help='N√∫mero de trials para optimizaci√≥n')
    parser.add_argument('--quick-test', action='store_true',
                        help='Ejecutar test r√°pido con 5 trials')

    args = parser.parse_args()

    pipeline = OptimizationPipeline(
        symbols=args.symbols,
        timeframe=args.timeframe,
        train_start="2025-01-01",
        train_end="2025-06-30",
        val_start="2025-07-01",
        val_end="2025-08-31",
        opt_start="2025-01-01",
        opt_end="2025-08-31",
        n_trials=args.trials
    )

    # Ejecutar pipeline
    try:
        if args.quick_test:
            results = pipeline.run_quick_test()
        else:
            results = await pipeline.run_complete_pipeline()

        logger.info("Pipeline ejecutado exitosamente")
        return results

    except Exception as e:
        logger.error(f"Error ejecutando pipeline: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
